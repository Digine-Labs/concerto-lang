# 17 - Runtime Engine

## Overview

The Concerto runtime is a stack-based virtual machine that executes IR generated by the compiler. It manages the execution loop, LLM connections, agent lifecycle, tool dispatch, memory hashmaps, the emit channel system, schema validation, error handling, and async execution.

## Architecture

```
                    +---------------------------+
                    |     Concerto Runtime      |
                    +---------------------------+
                    |                           |
  IR Input ------> |  IR Loader / Decoder      |
                    |         |                 |
                    |         v                 |
                    |  Instruction Dispatcher   |
                    |    |    |    |    |       |
                    |    v    v    v    v       |
                    | +----+----+----+----+    |
                    | |Val |Call|Agent|Tool|    |
                    | |Stk |Stk |Reg  |Reg |    |
                    | +----+----+----+----+    |
                    |    |    |    |    |       |
                    |    v    v    v    v       |
                    | +----+----+----+----+    |
                    | |Mem |Emit|Schema|Async|  |
                    | |Mgr |Chan|Valid |Exec |  |
                    | +----+----+----+----+    |
                    |         |                 |
                    |         v                 |
                    |    Connection Manager     |
                    |   (LLM API HTTP Client)   |
                    +---------------------------+
                              |
                              v
                     Emits to Host Application
```

## Components

### IR Loader

Deserializes the `.conc-ir` JSON file and validates its structure.

**Responsibilities:**
- Parse JSON IR
- Validate IR version compatibility
- Resolve constant pool references
- Build instruction dispatch tables
- Validate type definitions
- Register agents, tools, schemas, connections, hashmaps, pipelines

**Error handling:**
- Invalid JSON: `IRError::ParseError`
- Version mismatch: `IRError::VersionMismatch`
- Missing required sections: `IRError::MissingSection`

### Instruction Dispatcher

The main execution loop. Fetches instructions from the current function, dispatches by opcode.

```
loop {
    instruction = fetch_instruction(program_counter)
    match instruction.op {
        LOAD_CONST => { push(constants[instruction.arg]) }
        ADD => { b = pop(); a = pop(); push(a + b) }
        CALL_AGENT => { dispatch_agent_call(instruction) }
        EMIT => { dispatch_emit(instruction) }
        RETURN => { return pop() }
        ...
    }
    program_counter += 1
}
```

### Value Stack

The operand stack for expression evaluation. Holds runtime values.

**Value types in the VM:**
```
enum Value {
    Int(i64),
    Float(f64),
    String(String),
    Bool(bool),
    Nil,
    Array(Vec<Value>),
    Map(HashMap<String, Value>),
    Tuple(Vec<Value>),
    Struct { type_name: String, fields: HashMap<String, Value> },
    Option(Option<Box<Value>>),
    Result(Result<Box<Value>, Box<Value>>),
    AgentRef(AgentId),
    HashMapRef(HashMapId),
    Closure { params: Vec<String>, body: Vec<Instruction>, captured: Vec<Value> },
    AsyncHandle(TaskId),
}
```

### Call Stack

Tracks function call frames. Each frame contains:
- **Function name**: for error reporting
- **Program counter**: instruction pointer within the function
- **Local variables**: named variable storage for this scope
- **Value stack base**: stack pointer at frame entry (for cleanup on return)
- **Try block stack**: active error handler frames
- **Source span**: for error reporting

```
struct CallFrame {
    function: String,
    pc: usize,
    locals: HashMap<String, Value>,
    stack_base: usize,
    try_stack: Vec<TryFrame>,
    span: SourceSpan,
}
```

### Agent Registry

Manages agent instances, their configurations, and lifecycle.

**Responsibilities:**
- Initialize agents from IR agent definitions
- Bind agents to their connections
- Attach tools to agents
- Bind memory hashmaps
- Execute agent methods (route to Connection Manager)
- Apply decorators (retry, timeout, cache, log)
- Track per-agent token usage

### Tool Registry

Registers available tools and enforces permissions.

**Responsibilities:**
- Register tool definitions from IR
- Validate tool method signatures
- Execute tool methods when invoked
- Check permissions before execution
- Auto-generate JSON function schemas for LLM function-calling
- Route tool results back to LLM

### Connection Manager

Manages HTTP connections to LLM providers.

**Responsibilities:**
- Initialize connections from IR connection definitions
- Read API keys from environment variables
- Send chat completion requests to LLM APIs
- Handle streaming responses
- Implement retry logic (exponential backoff)
- Enforce rate limits (requests/minute, tokens/minute, concurrent)
- Track token usage and costs
- Support provider-specific features (OpenAI structured output, Anthropic tool use, etc.)

### Memory Manager

Manages in-memory hashmaps.

**Responsibilities:**
- Initialize hashmaps from IR hashmap definitions
- Provide thread-safe read/write access
- Support scoped views (namespace prefixing)
- Fire change events for reactive subscriptions
- Optional persistence (serialize to JSON on shutdown, load on startup)
- Garbage collection of unused entries (future)

### Emit Channel

The output system for host communication.

**Responsibilities:**
- Dispatch fire-and-forget emits to registered host listeners
- Handle bidirectional emit-await (suspend execution, wait for host response)
- Buffer emits in batch mode
- Manage channel subscriptions
- Serialize emit payloads to JSON for host consumption

### Schema Validator

Validates LLM responses against schema definitions.

**Responsibilities:**
- Parse LLM response text as JSON
- Validate against JSON Schema (compiled from Concerto schema definitions)
- Apply validation modes (strict, partial, coerce)
- Run custom validators
- Generate retry prompt messages on validation failure
- Support provider-native structured output where available

### Error Handler

Manages try/catch frames and error propagation.

**Responsibilities:**
- Push/pop try frames on the try stack
- On error: unwind call stack to nearest matching catch handler
- Support typed catch blocks (match error type)
- Implement `?` operator (PROPAGATE instruction)
- Attach error context for debugging
- Handle panics (unrecoverable -- terminate execution)

### Async Executor

Manages concurrent execution of async operations.

**Responsibilities:**
- Execute async agent calls without blocking the main loop
- Support parallel await (multiple concurrent operations)
- Support race (first to complete wins, cancel others)
- Integrate with the Connection Manager's rate limiter
- Manage task cancellation and timeouts

## Host API

The runtime is embedded in host applications as a Rust library.

### Basic Usage

```rust
use concerto_runtime::{Runtime, RuntimeConfig, Value};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Create runtime
    let mut runtime = Runtime::new(RuntimeConfig::default());

    // Load IR
    runtime.load_ir("program.conc-ir")?;

    // Register emit handlers
    runtime.on("result", |payload: Value| {
        println!("Result: {:?}", payload);
    });

    runtime.on("error", |payload: Value| {
        eprintln!("Error: {:?}", payload);
    });

    // Register bidirectional emit handler
    runtime.on_emit("tool:read_file", |payload: Value| async move {
        let path = payload.get_string("path")?;
        let content = tokio::fs::read_to_string(path).await?;
        Ok(Value::String(content))
    });

    // Execute
    runtime.execute().await?;

    // Get metrics
    let metrics = runtime.metrics();
    println!("Total tokens: {}", metrics.total_tokens());
    println!("Estimated cost: ${:.4}", metrics.estimated_cost());

    Ok(())
}
```

### Configuration

```rust
let config = RuntimeConfig {
    // Override connections for testing
    connection_overrides: HashMap::from([
        ("openai".to_string(), ConnectionOverride {
            api_key: Some("sk-test-key".to_string()),
            base_url: Some("http://localhost:8080/mock".to_string()),
            ..Default::default()
        }),
    ]),

    // Tool permissions
    tool_permissions: HashMap::from([
        ("ShellTool".to_string(), ToolPermission::Disabled),
        ("FileConnector".to_string(), ToolPermission::Methods(vec!["read_file".to_string()])),
    ]),

    // Emit mode
    emit_mode: EmitMode::Immediate,

    // Global timeout
    execution_timeout: Duration::from_secs(300),

    // Debug mode
    debug: false,
};
```

## Security

### Tool Sandboxing
- Tools are disabled by default; host must explicitly enable them
- `ShellTool` requires explicit opt-in
- File system tools can be restricted to specific directories
- Network tools can be restricted to specific domains

### Resource Limits
- Maximum execution time (configurable timeout)
- Maximum memory usage for hashmaps
- Maximum concurrent agent calls
- Maximum emit queue size

### API Key Protection
- Keys are read from environment variables, never stored in IR
- Connection configurations in IR use `api_key_env` (env var name), not the actual key
- Host can override connections to inject keys securely

## Debugging

### Step Execution
```rust
let mut runtime = Runtime::new_debugger(config);
runtime.load_ir("program.conc-ir")?;

// Set breakpoint
runtime.set_breakpoint("main", 5); // Function "main", instruction 5

// Step through
loop {
    let state = runtime.step().await?;
    println!("PC: {}, Stack: {:?}, Locals: {:?}", state.pc, state.stack, state.locals);
    if state.halted { break; }
}
```

### Variable Inspection
```rust
let locals = runtime.inspect_locals();
let stack = runtime.inspect_stack();
let agents = runtime.inspect_agents();
```

## Metrics

The runtime tracks execution metrics:

| Metric | Description |
|--------|-------------|
| `total_tokens` | Total tokens used (in + out) |
| `tokens_by_agent(name)` | Tokens per agent |
| `tokens_by_connection(name)` | Tokens per connection |
| `estimated_cost()` | Estimated cost based on model pricing |
| `execution_time_ms` | Total execution wall time |
| `agent_calls` | Number of LLM API calls |
| `tool_calls` | Number of tool invocations |
| `emit_count` | Number of emits produced |
| `errors` | Number of errors caught |

## CLI Interface

The runtime includes a CLI for direct execution:

```
# Execute an IR file
concerto run program.conc-ir

# Execute with environment file
concerto run program.conc-ir --env .env

# Execute with debug output
concerto run program.conc-ir --debug

# Execute with tool restrictions
concerto run program.conc-ir --disable-tool ShellTool

# Show metrics after execution
concerto run program.conc-ir --metrics
```
